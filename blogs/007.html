<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>from neuron to intelligence: introduction to neural networks</title>
    <meta name="description" content="an introduction to neural networks" />
    <meta
      name="keywords"
      content="programming, machine learning, neural networks, go, golang, from scratch, computer science"
    />
    <meta name="author" content="0xmukesh" />

    <link rel="stylesheet" href="/styles.css" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.16.19/dist/katex.min.css"
      integrity="sha384-7lU0muIg/i1plk7MgygDUp3/bNRA65orrBub4/OSWHECgwEsY83HaS1x3bljA/XV"
      crossorigin="anonymous"
    />

    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.16.19/dist/katex.min.js"
      integrity="sha384-RdymN7NRJ+XoyeRY4185zXaxq9QWOOx3O7beyyrRK4KQZrPlCDQQpCu95FoCGPAE"
      crossorigin="anonymous"
    ></script>

    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.16.19/dist/contrib/auto-render.min.js"
      integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);"
    ></script>
  </head>
  <body class="blog">
    <h1>from neuron to intelligence: introduction to neural networks</h1>
    <p class="blog__byline">
      by <a href="/">mukesh</a> on
      <time datetime="2025-01-08">8th January 2025</time>
    </p>

    <p>
      hey there! i have recently started exploring the world of machine learning
      and have been trying to implement a simple neural network from scratch
      (referring to "<a href="https://nnfs.io">Neural Networks from Scratch</a
      >"), so i thought of writing a blog series about neural networks, their
      working under-the-hood and how to implement one in golang.
    </p>

    <h2>what is machine learning?</h2>

    <p>
      before directly diving into neural networks, we have to get an idea about
      what machine learning actually is. if you are not living under a rock, you
      might have heard about the word "machine learning" or "artificial
      intelligence" at least once in the recent times. chatgpt,
      <a href="https://claude.ai" target="_blank">claude</a>,
      <a href="https://pplx.ai" target="_blank">perplexity</a> and
      <a href="https://gemini.google.com/">gemini</a> are all possible because
      of the magic of machine learning.
    </p>

    <p>
      machine learning is a field in computer science which deals with teaching
      computers to learn from <i>experiences</i> and make some predications,
      without being explicity programmed. whereas, artificial intelligence is a
      much broader classfication and it consists of various sub-domains such as
      machine learning (ML) and natural language processing (NLP).
    </p>

    <p>
      machine learning is all about different algorithms that are fed with some
      input data, do some math, and make predictions. for example, email
      providers like gmail use machine learning algorithms to filter your email
      and keep spam out of your inbox.
    </p>

    <p>
      on a high-level, the email spam filtering algorithms are trained (i.e.
      <i>fed with</i>) on a huge dataset of emails and each email is labeled as
      "spam" or "not spam". after analyzing, the algorithm eventually
      <i>learns</i> how to spot whether an email is spam or not by identifying
      some patterns.
    </p>

    <p>
      in the above example, the algorithm identifies whether a certain mail is
      spam or not spam i.e. classifies the input data into different categories.
      this is an example of <b>classification</b>. in classification, the
      algorithm predicts which category something belongs to based on its
      characteristics.
    </p>

    <p>
      in case of weather forecasting, the algorithm takes in different input
      data such as temperature history, humidity, wind speed and pressure and
      predicts the probable temperature. this is an example of
      <b>regression</b>. in regression, the algorithm predicts a continuous
      numerical value based on input data.
    </p>

    <p>
      until now, we have <i>fed</i> the algorithms with input data which
      <i>trains</i> them, but there are other ways of training as well. let's
      briefly go through the most popular ones:
    </p>

    <ol>
      <li>supervised learning</li>
      <li>unsupervised learning</li>
      <li>reinforcement learning</li>
    </ol>

    <p>
      in supervised learning, the algorithm is presented with a dataset of
      inputs along with their desired outputs (labels). for example, to design a
      machine learning algorithm that categorizes images of humans and cats, the
      algorithm is fed with thousands of images of humans and cats and each
      image is labeled as either "human" or "cat".
    </p>

    <p>
      in unsupervised learning, the algorithm is presented with a dataset of
      inputs but with their desired outputs, which means the algorithm must find
      structure, patterns and group similar ones all by itself.
    </p>

    <p>
      in reinforcement learning, the algorithm performs an action in an
      environment and then recieves a feedback, basically "learning from
      mistakes".
    </p>

    <h2>what are neural networks?</h2>

    <p>
      neural networks, also known as artifical neural networks (ANNs), is a type
      of machine learning algorithm that is inspired by the biological brain.
      neural networks approach various problems by trying to mimic how neurons
      in the human brain work. they are composed of large number of highly
      interconnected elements (nodes) that work in parallel to solve a specific
      problem.
    </p>

    <p>
      there are different types of neural networks, each specialized for a
      certain task such as speech recognition or image recognition.
    </p>

    <ol>
      <li>
        convolutional neural network (CNN) is commonly used for image
        recognition.
      </li>

      <li>
        long short-term memory neural network (LSTM) is commonly used for speech
        recognition.
      </li>

      <li>
        recurrent neural network (RNN) is specialized in sequential data
        processing i.e. it can make sequential predications based on sequential
        inpts.
      </li>
    </ol>

    <h2>elements of a neural network</h2>

    <p>
      irrespective of complexity of various neural networks, they all have the
      same fundamental basis. every neural network is built on top of the
      following elements - neurons, weights and biases.
    </p>

    <p>
      <b>neurons</b> are the "highly interconnected nodes" of the neural
      network. neurons act like the processing units in a neural network,
      receiving inputs from other neurons, performing caluclations and then
      producing the output that is passed on to the other connected neurons.
    </p>

    <br />

    <center>
      <img src="../images/007/01.svg" style="width: 50%; height: auto" />
    </center>

    <br />

    <p>
      in the above image, a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>, y<sub
        >1</sub
      >
      and y<sub>2</sub> are the neurons. a<sub>1</sub>, a<sub>2</sub> and a<sub
        >3</sub
      >
      recieve the training data (aka input data) which is then processed and
      sent to y<sub>1</sub>
      and y<sub>2</sub>.
    </p>

    <p>
      a collection of neurons form a layer. in the above image, there are two
      input and output layers. the input layer consists of a<sub>1</sub>, a<sub
        >2</sub
      >
      and a<sub>3</sub> whereas the output layer consists of b<sub>1</sub> and
      b<sub>2</sub>. the neurons in the current layer affect the value of
      neurons in the next layer i.e. the value of y<sub>1</sub> depends on
      a<sub>1</sub>, a<sub>2</sub> and a<sub>3</sub> and similarly for y<sub
        >2</sub
      >
      as well.
    </p>

    <p>
      <b>weight</b> determines how much a neuron in the current layer affects
      the neuron in the next layer. each connection of neurons has a specific
      weight assigned to it.
    </p>

    <center>
      <img src="../images/007/02.svg" style="width: 50%; height: auto" />
    </center>

    <p>in the above figure, the weights in the following fashion:</p>

    <ol>
      <li>a<sub>i</sub> - y<sub>1</sub> connection - w<sub>i</sub></li>
      <li>a<sub>j</sub> - y<sub>2</sub> connection - v<sub>j</sub></li>
    </ol>

    <p>
      the value of y<sub>1</sub> is determined by taking weighted sum of all the
      neurons which are connected to it. in the above example, it would be
      calculated as follows:
    </p>

    <p>
      $$\begin{aligned} y_1 &= a_1w_1 + a_2w_2 + a_3w_3 \\ &= \sum_{i = 1}^3
      a_iw_i \end{aligned}$$
    </p>

    <p>and similarly for y<sub>2</sub></p>

    <p>
      $$\begin{aligned} y_2 &= a_1v_1 + a_2v_2 + a_3v_3 \\ &= \sum_{j = 1}^3
      a_jv_j \end{aligned}$$
    </p>

    <p>
      currently, our neural network is based on a simple linear equation of the
      form y = mx but what if we want to offset the result? i.e. if the result
      of the weighted sum is 0 then we want the neural network to return 2. we
      would have to add an offset constant to our above-mentioned linear
      equation, which makes it y = mx + c.
    </p>

    <p>
      the additional +c in the equation is the bias. <b>bias</b> is used to
      offset the value of the weighted sum.
    </p>

    <center>
      <img src="../images/007/03.svg" style="width: 50%; height: auto" />
    </center>

    <p>
      in the above image, b<sub>1</sub> and b<sub>2</sub> are the biases for
      y<sub>1</sub> and y<sub>2</sub>. every neuron has its' own biases -
      similar to how every neuron connection has its' own weight.
    </p>

    <p>
      to calculate the value of y<sub>1</sub>, we have to just add b<sub>1</sub>
      to the previously calculated weighted sum.
    </p>

    <p>
      $$\begin{aligned} y_1 &= (a_1w_1 + a_2w_2 + a_3w_3) + b_1 \\ &=
      \left(\sum_{i = 1}^3 a_iw_i \right) + b_1 \end{aligned}$$
    </p>

    <p>and similarly for y<sub>2</sub></p>

    <p>
      $$\begin{aligned} y_2 &= (a_1v_1 + a_2v_2 + a_3v_3) + b_2 \\ &=
      \left(\sum_{j = 1}^3 a_jv_j \right) + b_2 \end{aligned}$$
    </p>

    <p>
      let's general the formula using a bit of linear algebra. let's consider a
      few matrices - <code>A</code>, <code>W</code>, <code>B</code> and
      <code>Y</code>.
    </p>

    <ol>
      <li>
        <code>A</code> is a row matrix of all the input neurons (also called as
        "features"). the shape of the matrix ("shape of a matrix" basically
        means dimensions of that matrix) is 1xn, where n is equal to number of
        neurons in the current layer.

        <p>
          $$\text{A} = \left(\begin{matrix} a_1 & a_2 & a_3 \end{matrix}
          \right)$$
        </p>
      </li>

      <li>
        <code>W</code> is a matrix of weights of all the neuron connections.
        weights present in the <code>i</code>th column of the matrix corresponds
        to weights related to a<sub>i</sub>. the shape of the matrix is mxn,
        where m is the number of neurons in current layer and n is the number of
        neurons in the next layer.

        <p>
          $$\text{W} = \left(\begin{matrix} w_1 & v_1 \\ w_2 & v_2 \\ w_3 & v_3
          \end{matrix} \right)$$
        </p>

        <p>
          w<sub>1</sub>, w<sub>2</sub> and w<sub>3</sub> are present in the 1st
          column, thereby they are the weights related to a<sub>1</sub> and
          similarly for v<sub>1</sub>, v<sub>2</sub> and v<sub>3</sub>.
        </p>
      </li>

      <li>
        <code>B</code> is also a row matrix of all the biases of neurons in the
        next layer. the shape of the matrix is 1xn, where n is the number of
        neurons in the next layer.

        <p>
          $$\text{B} = \left(\begin{matrix} b_1 & b_2 \end{matrix} \right)$$
        </p>
      </li>

      <li>
        <code>Y</code> is also a row matrix containing values of all the neurons
        in next layer. the shape of the matrix is 1xn, where n is the number of
        neurons in the next layer.
      </li>

      <p>$$\text{Y} = \left(\begin{matrix} y_1 & y_2 \end{matrix} \right)$$</p>
    </ol>

    <p>
      using linear algebra, we can find relation between <code>A</code>,
      <code>W</code>, <code>B</code> and <code>Y</code>
    </p>

    <p>$$\text{Y} = \text{A}\text{W} + \text{B}$$</p>

    <h2>coding our first neurons</h2>

    <p>
      let's code out a simple neural network with two layers (input and output
      layer) in golang. the input layer consists of 3 neurons and the output
      layer consists of 2 neurons.
    </p>

    <p>
      start with defining <code>A</code>, <code>W</code> and
      <code>B</code> matrices. for now, use any random values to populate the
      matrices.
    </p>

    <pre><code class="language-go">package main

func main() {
  A := []float64{1, 2, 3}
  W := [][]float64{
    {0.5, 0.7},
    {0.4, 0.6},
    {0.3, 0.2},
  }
  B := []float64{1, 0.9}
}</code></pre>

    <p>
      let's start with implementing the matrix multiplication between
      <code>A</code> and <code>W</code>.
    </p>

    <pre><code class="language-go">AxW := make([][]float64, len(A))

for i := range AxW {
  AxW[i] = make([]float64, len(W[0]))
}

for i := 0; i < len(A); i++ {
  for j := 0; j < len(W[0]); j++ {
    sum := 0.0

    for k := 0; k < len(W); k++ {
      sum += A[i][k] * W[k][j]
    }

    AxW[i][j] = sum
  }
}</code></pre>

    <p>
      the code starts off with allocating the required size for the result
      matrix i.e. <code>AxW</code>. the shape of <code>AxW</code> matrix is mxn,
      where m is equal to number of rows of A and n is the number of columns of
      W.
    </p>

    <p>
      the code then loops through every row in <code>A</code> and every column
      in <code>W</code>, and then it multiplies the corresponding elements and
      calculates the total sum.
    </p>

    <p>
      the above code assumes that both the matrices are multipliable with each
      other. for two matrices, the number of columns in the first matrix must be
      equal to number of rows in the second matrix.
    </p>

    <p>
      let's now implement the matrix addition between <code>AxW</code> and
      <code>B</code>.
    </p>

    <pre><code class="language-go">result := make([][]float64, len(AxW))

for i := range result {
  result[i] = make([]float64, len(AxW[0]))
}

for i := 0; i < len(AxW); i++ {
  for j := 0; j < len(AxW[0]); j++ {
    result[i][j] = AxW[i][j] + B[i][j]
  }
}</code></pre>

    <p>
      the code starts off with allocating the required size for the result
      matrix. the shape of <code>result</code> matrix is equal to the shape of
      <code>AxW</code> matrix, as two matrices can be added/subtracted only when
      their shapes (dimensions) are equal.
    </p>

    <p>here's the entire code until now:</p>

    <pre><code class="language-go">package main

import "fmt"

func main() {
  A := [][]float64{{1, 2, 3}}
  W := [][]float64{
    {0.5, 0.7},
    {0.4, 0.6},
    {0.3, 0.2},
  }
  B := [][]float64{{1, 0.9}}

  AxW := make([][]float64, len(A))

  for i := range AxW {
    AxW[i] = make([]float64, len(W[0]))
  }

  for i := 0; i < len(A); i++ {
    for j := 0; j < len(W[0]); j++ {
      sum := 0.0

      for k := 0; k < len(W); k++ {
        sum += A[i][k] * W[k][j]
      }

      AxW[i][j] = sum
    }
  }

  result := make([][]float64, len(AxW))

  for i := range result {
    result[i] = make([]float64, len(AxW[0]))
  }

  for i := 0; i < len(AxW); i++ {
    for j := 0; j < len(AxW[0]); j++ {
      result[i][j] = AxW[i][j] + B[i][j]
    }
  }

  fmt.Println(result)
}</code></pre>

    <p>
      on running the script, it must print out <code>[[3.2 3.4]]</code>. you can
      verify the answer via a calculator or solving the matrices by hand.
    </p>

    <h3>refactoring</h3>

    <p>
      the current implementation works but it is pretty rough and has a lot of
      areas of failure. let's make a few utility functions related to different
      matrix operations so that we don't have to write out the logic from
      scratch, using <code>for</code> loop, every time we want to do that
      specific matrix operation.
    </p>

    <p>
      let's create a new distinct type named <code>Matrix</code>, on which we
      will define our utility functions as custom methods.
    </p>

    <pre><code class="language-go">package math

type Matrix [][]float64</code></pre>

    <p>
      over here, we are creating a new distinct type and not a type alias. to
      define a type alias, we have to add <code>=</code> operator in between.
    </p>

    <pre><code class="language-go">type Matrix = [][]float64</code></pre>

    <p>
      let's start with methods which return metadata related to the matrix i.e.
      return number of rows and columns.
    </p>

    <pre><code class="language-go">func (m Matrix) Rows() int {
  return len(m)
}

func (m Matrix) Cols() int {
  return len(m[0])
}</code></pre>

    <p>
      let's also add another helper function which allocates memory for a matrix
      with given number of rows and columns.
    </p>

    <pre><code class="language-go">func AllocateMatrix(rows, cols int) Matrix {
  matrix := make(Matrix, rows)
  for i := range matrix {
    matrix[i] = make([]float64, cols)
  }

  return matrix
}</code></pre>

    <p>now let's utilize these functions to implement matrix addition.</p>

    <pre><code class="language-go">func (m Matrix) Add(n Matrix) Matrix {
  if (m.Rows() != n.Rows()) || (m.Cols() != n.Cols()) {
    panic("invalid dimensions")
  }

  result := AllocateMatrix(m.Rows(), n.Cols())

  for i := 0; i < m.Rows(); i++ {
    for j := 0; j < n.Cols(); j++ {
      result[i][j] = m[i][j] + n[i][j]
    }
  }

  return result
}</code></pre>

    <p>
      the code is pretty much the same as the barebones for-loop which we wrote
      initially. the only additional thing that is added is checking if both the
      matrices can be added or not i.e. checking if their dimensions are equal.
    </p>

    <p>let's also implement matrix multiplication.</p>

    <pre><code class="language-go">func (m Matrix) Multiply(n Matrix) Matrix {
  if m.Cols() != n.Rows() {
    panic(fmt.Errorf("invalid operation, tried to multiply %dx%d matrix with %dx%d matrix", m.Rows(), m.Cols(), n.Rows(), n.Cols()))
  }

  result := AllocateMatrix(m.Rows(), n.Cols())

  for i := 0; i < m.Rows(); i++ {
    for j := 0; j < n.Cols(); j++ {
      sum := 0.0

      for k := 0; k < n.Rows(); k++ {
        sum += m[i][k] * n[k][j]
      }

      result[i][j] = sum
    }
  }

  return result
}</code></pre>

    <p>
      the code is pretty much the same except for the fact that the it now also
      checks whether both the matrices are multipliable by checking if number of
      columns of the 1st matrix is equal to number of rows of the 2nd matrix.
    </p>

    <p>time to use these utility functions to refactor <code>main.go</code>.</p>

    <pre><code class="language-go">package main

import (
  "fmt"
  m "nn/math"
)

func main() {
  A := m.Matrix{{1, 2, 3}}
  W := m.Matrix{
    {0.5, 0.7},
    {0.4, 0.6},
    {0.3, 0.2},
  }
  B := m.Matrix{{1, 0.9}}

  result := A.Multiply(W).Add(B)
  fmt.Println(result)
}</code></pre>

    <p>
      the code looks much more cleaner and elegant! and it is also much easier
      to understand what is exactly happening.
    </p>

    <h2>multilayers</h2>

    <center>
      <img src="../images/007/04.svg" style="width: 65%; height: auto" />
    </center>

    <p>
      until now we have only worked with single hidden layer, which was the
      output layer. let's add in another layer.
    </p>

    <p>
      there is a slight change in naming convention - the weights of first layer
      are denoted as w<sub>ij</sub> where <code>i</code> is the index of neuron
      in the current layer and <code>j</code> is the index of neuron of in the
      next layer and similarly for weights of second layer but it is denoted as
      v<sub>ij</sub>.
    </p>

    <p>
      adding more layers to a neural network, allows the network to learn more
      complex patterns and relationships within the input data.
    </p>

    <p>
      implementing it in code is pretty easy. the output of first hidden layer
      is the input of second hidden layer, so in place of <code>A</code>, we'll
      use <code>result1</code> as the input while calculating values in the
      output layer (second hidden layer). <code>W1</code> and
      <code>B1</code> are the weights and biases corresponding to first hidden
      layer and similarly for <code>W2</code> and <code>B2</code>.
    </p>

    <pre><code class="language-go">package main

import (
  "fmt"
  m "nn/math"
)

func main() {
  A := m.Matrix{{1, 2, 3}}
  W1 := m.Matrix{
    {0.5, 0.7},
    {0.4, 0.6},
    {0.3, 0.2},
  }
  B1 := m.Matrix{{1, 0.9}}
  W2 := m.Matrix{
    {0.7, 0.9},
    {-0.4, 0.3},
  }
  B2 := m.Matrix{{1.1, -0.9}}

  result1 := A.Multiply(W1).Add(B1)
  result2 := result1.Multiply(W2).Add(B2)
  fmt.Println(result2)
}</code></pre>

    <p>lovely! but let's refactor the code a bit.</p>

    <h3>denselayer struct</h3>

    <p>
      let's create a layer of abstraction which <i>generates</i> a hidden layer
      with random weights and biases based on number of input neurons and number
      of output neurons.
    </p>

    <pre><code class="language-go">package neural

import (
  m "nn/math"
)

type DenseLayer struct {
  Input, Weights, Bias m.Matrix
  NInputs, NNeurons    int
}</code></pre>

    <p>
      <code>NInputs</code> is the number of input neurons and
      <code>NNeurons</code> is the number of output neurons.
    </p>

    <pre><code class="language-go">func NewDenseLayer(nInputs, nNeurons int) *DenseLayer {
  dl := &DenseLayer{}

  dl.Weights = m.AllocateMatrix(nInputs, nNeurons)
  dl.Bias = m.AllocateMatrix(1, nNeurons)

  for i := range dl.Weights {
    for j := range dl.Weights[i] {
      dl.Weights[i][j] = rand.Float64()*2 - 1
    }
  }

  dl.NInputs = nInputs
  dl.NNeurons = nNeurons

  return dl
}</code></pre>

    <p>
      <code>NewDenseLayer</code> acts like the constructor for
      <code>DenseLayer</code> struct. it returns a new denselayer with randomly
      generated weights, in the range of [-1, 1), and zero'ed biases, and sets
      the value of <code>NInputs</code> and <code>NNeurons</code>.
    </p>

    <p>
      <code>rand.Float64</code> returns a random float in the range of [0, 1),
      so to generate a float in the range of [-1, 1), we have multiplied it by 2
      and then subtracted 1 from it - a common technique for mapping elements in
      range of [0, 1) to [-1, 1).
    </p>

    <p>
      until now, we have been processing the input data and passing it to the
      next layer. this is called as <b>forward pass</b>. during forward pass,
      the input data moves from the input layer through the hidden layers to the
      output layer.
    </p>

    <pre><code class="language-go">func (dl *DenseLayer) Forward(input m.Matrix) m.Matrix {
  return input.Multiply(dl.Weights).Add(dl.Bias)
}</code></pre>

    <p>
      <code>Forward</code> method performs a forward pass on that specific
      hidden layer, basically returns sum of the weighted sum and biases.
    </p>

    <p>let's utilize this new layer of abstraction in <code>main.go</code>.</p>

    <pre><code class="language-go">package main

import (
  "fmt"
  m "nn/math"
  n "nn/neural"
)

func main() {
  input := m.Matrix{{1, 2, 3}}
  dl1 := n.NewDenseLayer(3, 2)
  dl2 := n.NewDenseLayer(2, 2)
  result1 := dl1.Forward(input)
  result2 := dl2.Forward(result1)

  fmt.Println(result2)
}</code></pre>

    <p>
      the above code performs a forward pass on a neural network with 3 layers -
      input layer with 3 neurons, hidden layer with 2 neurons and output layer
      with 2 neurons.
    </p>

    <h2>batch inputs</h2>

    <center>
      <img src="../images/007/05.svg" style="width: 65%; height: auto" />
    </center>

    <br />

    <center>
      <img src="../images/007/06.svg" style="width: 65%; height: auto" />
    </center>

    <br />

    <p>
      neither rome is built within a single iteration nor a neural network is
      trained with a single batch of input data. (sorry for the bad joke)
    </p>

    <p>
      we have to tweak our forward pass logic to support batch inputs, as the
      one mentioned above, and return individual outputs for each set of inputs.
    </p>

    <pre><code class="language-go">// expected input
input := m.Matrix{
  {1, 2, 3},
  {4, 5, 6},
}

// expected output
output := m.Matrix{
  {0.4, -0.56},
  {-7.89, 0.98},
}</code></pre>

    <p>
      but just adding another row to the input matrix wouldn't work, as in our
      forward pass method, bias is a row matrix, and with batched inputs, the
      weighted sum wouldn't be a row matrix as earlier, and two matrices of
      different dimensions can't be added.
    </p>

    <p>
      a solution for this is to loop through the input matrix, where the type of
      each element is <code>[]float64</code>. convert the elements from
      <code>[]float64</code> to a row matrix, perform the calculations, convert
      it back to <code>[]float64</code> and append it to a result matrix.
    </p>

    <p>
      let's code out the utility functions related to conversion between
      <code>[]float64</code> and row matrix.
    </p>

    <pre><code class="language-go">func ToRowMatrix(v []float64) Matrix {
  m := AllocateMatrix(1, len(v))

  for i := range m {
    for j := range m[i] {
      m[i][j] = v[j]
    }
  }

  return m
}</code></pre>

    <pre><code class="language-go">func (m Matrix) ToFloatSlice() []float64 {
  if len(m) != 1 {
    panic("not a row matrix")
  }

  return m[0]
}</code></pre>

    <p>let's use these utility functions while performing forward pass.</p>

    <pre><code class="language-go">func (dl *DenseLayer) Forward(input m.Matrix) m.Matrix {
  result := m.AllocateMatrix(len(input), dl.NNeurons)

  for i, v := range input {
    result[i] = m.ToRowMatrix(v).Multiply(dl.Weights).Add(dl.Bias).ToFloatSlice()
  }

  return result
}</code></pre>

    <p>
      the shape of input matrix is mxn, where m is the number of batches and n
      is the number of input neurons (features). the shape of result matrix is
      mxn, where m is the number of batches and n is the number of output
      neurons.
    </p>

    <p>
      update the input to the one as mentioned above and run
      <code>main.go</code> again. the output must contain 2 rows and within each
      row, there must be 2 values.
    </p>

    <h2>activation functions</h2>

    <p>
      with the current architecture of our neural network (multi-layer
      perceptron), it can only model the predications with linear functions
      irrespective of number of hidden layers. if f(x) is linear then
      f(f(f(f(x)))) is also linear.
    </p>

    <p>
      in the below image, the neural network is trying to classify objects into
      two different categories represented by red and blue dots.
    </p>

    <center>
      <img src="../images/007/07.svg" style="width: 40%; height: auto" />
    </center>

    <p>
      the network might have high accuracy for simple training data, but as the
      training data gets more complicated, the network fails to model the
      predications properly using just linear functions.
    </p>

    <center>
      <img src="../images/007/08.svg" style="width: 40%; height: auto" />
    </center>

    <p>
      notice how the network fails to model the predications using just linear
      functions. the network labels a decent chunk of objects belonging to red
      category as blue.
    </p>

    <p>
      activation functions are the components of the neural network that
      introduce non-linearity into the network, allowing the networks to learn
      more complex patterns and relationships. notice the change between how the
      network models the predications with help of activation function and with
      just linear functions. activation functions also determine whether a
      neuron is <i>activated</i> or not and by much as well.
    </p>

    <center>
      <img src="../images/007/09.svg" style="width: 40%; height: auto" />
    </center>

    <p>
      notice how the network is able to model the predications with much more
      accuracy as compared to with just linear functions.
    </p>

    <p>
      there are different types of activation functions, so let's go through few
      of most commonly used ones:
    </p>

    <ol>
      <li>unit step function</li>
      <li>sigmoid</li>
      <li>tanh</li>
      <li>ReLU</li>
      <li>leaky ReLU</li>
      <li>softmax</li>
    </ol>

    <h3>unit step function</h3>

    <p>
      $$f(x) = \left\{ \begin{array}{ll} 1 & \text{if } x > 0 \\ 0 & \text{if }
      x \leq 0 \end{array} \right. $$
    </p>

    <p>
      >> check out the graph of step function on
      <a href="https://www.desmos.com/calculator/iu4hww4zeq" target="_blank"
        >desmos</a
      >
    </p>

    <p>
      the unit step function is a discontinous function which models the on/off
      behavior of a switch into the network. it is also known as the
      <a href="https://en.wikipedia.org/wiki/Oliver_Heaviside">heaviside</a>
      function.
    </p>

    <p>
      if the value of a neuron is less than or equal to 0 then it is not
      activated else it is activated. the unit step function is one of the most
      basic activation function and it clearly shows the concept of a neuron
      being activated.
    </p>

    <p>
      but unit step function isn't generally used as an activation function.
      while training our neural network, the optimizer (the component
      responsible for training the network) needs to also know
      <i>how much</i> of the neuron is activated i.e. how close or far is the
      neuron from the threshold.
    </p>

    <p>
      with unit step function, the output is either 1 or 0 so the question of
      "<i>how much</i>" can't really be answered with the help of unit step
      function. thereby, making unit step function less informative, in terms of
      being an activation functions.
    </p>

    <h3>sigmoid</h3>

    <p>$$\sigma(x) = \frac{1}{1 + e^{-x}}$$</p>

    <p>
      >> check out the graph of sigmoid function on
      <a href="https://www.desmos.com/calculator/0jaesaq1jv" target="_blank"
        >desmos</a
      >
    </p>

    <p>
      the sigmoid function (also called as logistic function) is a S-shaped
      monotonically increasing function. the major difference between the unit
      step function and sigmoid function is that sigmoid is monotonically
      increasing.
    </p>

    <p>
      due to the monotonically increasing behaviour of the sigmoid function, the
      optimizer can judge how close or far is the neuron from the threshold.
    </p>

    <p>the range of sigmoid function is (0, 1) (where 0 and 1 are exclusive)</p>

    <p>
      $$\sigma(x) = 0, \text{when} \; x = -\infty \\ \sigma(x) = 1, \text{when}
      \; x = \infty$$
    </p>

    <p>
      sigmoid function is a common choice as an activation function when
      probability distributions are expected in the output layer as the input to
      the function is transformed into a value between 0.0 and 1.0.
    </p>

    <h3>tanh</h3>

    <p>
      $$\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1$$
    </p>

    <p>
      >> check out the graph of tanh on
      <a href="https://www.desmos.com/calculator/h1zo4m8kgl" target="_blank"
        >desmos</a
      >
    </p>

    <p>
      tangent hyperbolic or tanh function is another common choice as an
      activation function in neural networks. tanh is a shifted and stretched
      version of the sigmoid.
    </p>

    <p>
      the range of tanh is -1.0 to 1.0, as compared to 0.0 to 1.0 in the case of
      sigmoid. generally, tanh is preferred over sigmoid as it performs better.
    </p>

    <p>
      but there is an issue with both tanh and sigmoid and that is
      <b>vanishing gradient problem</b>. i won't be going too deep into this
      topic as it requires the knowledge of backpropagation. i will be covering
      this topic along with backpropagation in the next part of this series.
    </p>

    <h3>ReLU</h3>

    <p>
      $$f(x) = \left\{ \begin{array}{ll} x & \text{if } x > 0 \\ 0 & \text{if }
      x \leq 0 \end{array} \right. = \text{max}(0, \; x) $$
    </p>

    <p>
      >> check out the graph of ReLU on
      <a href="https://www.desmos.com/calculator/acf2vxkmdn" target="_blank"
        >desmos</a
      >
    </p>

    <p>
      rectified linear activation function or ReLU is a simple yet effective
      non-linear activation function. if a value is greater than 0 then it
      outputs it as it is else it outputs 0.
    </p>

    <p>
      ReLU addresses the vanishing gradient problem, making it effective in deep
      learning architectures.
    </p>

    <p>
      but there is a problem with ReLU as well - <b>dying ReLU problem</b>. as
      this topic also requires knowledge of backpropagation, i would be covering
      this topic, along with vanishing gradient problem, in the next blog post
      in this series.
    </p>

    <p>
      but here is a simple explaination of dying ReLU problem which is enough to
      give you the context to understand why leaky ReLU exists.
    </p>

    <p>
      the dying ReLU problem occurs when neurons using the ReLU activation
      function become permanently inactive during neural network training (i
      will deep dive into what neural network training is and how does it happen
      in later blog posts). these neurons consistenly output 0 for any input,
      effectively <i>dying</i> and stopping their "learning" process.
    </p>

    <h3>leaky ReLU</h3>

    <p>
      $$f(x) = \left\{ \begin{array}{ll} x & \text{if } x > 0 \\ 0.01x &
      \text{if } x \leq 0 \end{array} \right. = \text{max}(0.01x, \; x)$$
    </p>

    <p>
      >> check out the graph of leaky ReLU on
      <a href="https://www.desmos.com/calculator/m3cxjrlvtc">desmos</a>
    </p>

    <p>
      leaky ReLU came into existence to fix the issue of the dying ReLU problem.
      rather than outputting 0 which leads to dead neurons, the leaky ReLU
      output that value multiplied by a tiny fraction such as 0.01.
    </p>

    <h3>softmax</h3>

    <p>$$S(x_i) = \frac{e^{x_i}}{\sum_{j = 1}^{n} e^{x_j}}$$</p>

    <p>
      >> check out the visualization of softmax on
      <a href="https://www.desmos.com/calculator/arep8wx65j">desmos</a>
    </p>

    <p>
      the softmax function is a special one as it doesn't take in a single input
      but rather a vector of inputs and returns a vector of probabilities.
      \(S(x_i)\) indicates the probability for \(x_i\) element in the input
      vector.
    </p>

    <p>
      $$\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} \to S(x) \to \begin{bmatrix}
      0.09 \\ 0.245 \\ 0.665 \end{bmatrix}$$
    </p>

    <p>
      the softmax function is used in the output layer of the neural network to
      convert the raw output scores into probabilities. with the help of the
      softmax, the output values are always in the range of (0, 1) and sum up to
      1, making them interpretable as probabilities.
    </p>

    <footer>
      <a href="./index.html"><p>&leftarrow; go back</p></a>
    </footer>

    <script src="/js/blogs/add-top-nav.js"></script>
    <script>
      hljs.highlightAll();

      renderMathInElement(document.body, {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "\\[", right: "\\]", display: true },
          { left: "$", right: "$", display: false },
          { left: "\\(", right: "\\)", display: false },
        ],
      });
    </script>
  </body>
</html>
